{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The junctions are loaded from the following path: /gpfs/commons/home/kisaev/LeafletSC/data/raw/junctions/\n",
      "The files in the path are: ['B107926_O8_Blue_Blood_S250.homo.gencode.v30.ERCC.chrM.juncswbarcodes', 'B107925_B5_S284.homo.gencode.v30.ERCC.chrM.juncswbarcodes']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import polars as pl\n",
    "\n",
    "# Define path that contains some junction files (only 2 files are used for this example, corresponding to 2 individual cells)\n",
    "juncs_path = \"/gpfs/commons/home/kisaev/LeafletSC/data/raw/junctions/\"\n",
    "print(\"The junctions are loaded from the following path: \" + juncs_path) \n",
    "\n",
    "# print the files in the path \n",
    "print(\"The files in the path are: \" + str(os.listdir(juncs_path)))\n",
    "\n",
    "# define path for saving the output data \n",
    "output_path = \"/gpfs/commons/home/kisaev/LeafletSC/data/processed/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pyranges as pr\n",
    "from gtfparse import read_gtf #initially tested with version 1.3.0)\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "import glob \n",
    "import time\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "import concurrent.futures\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'concurrent' from '/gpfs/commons/home/kisaev/miniconda3/envs/LeafletSC/lib/python3.10/concurrent/__init__.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# figure out version of concurrent.futures \n",
    "concurrent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_gtf(gtf_file): #make this into a seperate script that processes the gtf file into gr object that can be used in the main scriptas input \n",
    "    \"\"\"\n",
    "    Process the GTF file into a pyranges object.\n",
    "\n",
    "    Parameters:\n",
    "    - gtf_file (str): Path to the GTF file.\n",
    "\n",
    "    Returns:\n",
    "    - gtf_exons_gr (pyranges.GenomicRanges): Processed pyranges object.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"The gtf file you provided is \" + gtf_file)\n",
    "    print(\"This step may take a while depending on the size of your gtf file\")\n",
    "\n",
    "    # calculate how long it takes to read gtf_file and report it \n",
    "    start_time = time.time()\n",
    "    #[1] extract all exons from gtf file provided \n",
    "    gtf = read_gtf(gtf_file, result_type=\"pandas\") #to reduce the speed of this, can just get rows with exon in the feature column (preprocess this before running package)? check if really necessary\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(\"Reading gtf file took \" + str(round((end_time-start_time), 2)) + \" seconds\")\n",
    "    # assert that gtf is a non empty dataframe otherwise return an error\n",
    "    if gtf.empty or type(gtf) != pd.DataFrame:\n",
    "        raise ValueError(\"The gtf file provided is empty or not a pandas DataFrame. Please provide a valid gtf file and ensure you have the \\\n",
    "                         latest version of gtfparse installed by running 'pip install gtfparse --upgrade'\")\n",
    "    \n",
    "    # Convert the seqname column to a string in gtf \n",
    "    gtf[\"seqname\"] = gtf[\"seqname\"].astype(str)\n",
    "\n",
    "    # Make a copy of the DataFrame\n",
    "    gtf_exons = gtf[(gtf[\"feature\"] == \"exon\")].copy()\n",
    "\n",
    "    if gtf_exons['seqname'].str.contains('chr').any():\n",
    "        gtf_exons.loc[gtf_exons['seqname'].str.contains('chr'), 'seqname'] = gtf_exons['seqname'].map(lambda x: x.lstrip('chr').rstrip('chr'))\n",
    "\n",
    "    if not set(['seqname', 'start', 'end', 'score', 'strand', 'gene_id', 'gene_name', 'transcript_id', 'exon_id']).issubset(gtf_exons.columns):\n",
    "        # print the columns that the file is missing\n",
    "        missing_cols = set(['seqname', 'start', 'end', 'score', 'strand', 'gene_id', 'gene_name', 'transcript_id', 'exon_id']).difference(gtf_exons.columns)\n",
    "        print(\"Your gtf file is missing the following columns: \" + str(missing_cols))\n",
    "\n",
    "        # if the missing column is just exon_id, we can generate it\n",
    "        if \"exon_id\" in missing_cols:\n",
    "            # add exon_id to gtf_exons\n",
    "            print(\"Adding exon_id column to gtf file\")\n",
    "            gtf_exons.loc[:, \"exon_id\"] = gtf_exons[\"transcript_id\"] + \"_\" + gtf_exons[\"start\"].astype(str) + \"_\" + gtf_exons[\"end\"].astype(str)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # Convert the DataFrame to a PyRanges object\n",
    "    gtf_exons_gr = pr.from_dict({\"Chromosome\": gtf_exons[\"seqname\"], \"Start\": gtf_exons[\"start\"], \"End\": gtf_exons[\"end\"], \"Strand\": gtf_exons[\"strand\"], \"gene_id\": gtf_exons[\"gene_id\"], \"gene_name\": gtf_exons[\"gene_name\"], \"transcript_id\": gtf_exons[\"transcript_id\"], \"exon_id\": gtf_exons[\"exon_id\"]})\n",
    "\n",
    "    # Remove rows where exon start and end are the same or when gene_name is empty\n",
    "    gtf_exons_gr = gtf_exons_gr[ ~ (gtf_exons_gr.Start == gtf_exons_gr.End)]\n",
    "    gtf_exons_gr = gtf_exons_gr[ ~ (gtf_exons_gr.gene_name == \"\")]\n",
    "\n",
    "    # When do I need to do this? depends on gtf file used? base 0 or 1? probably need this to be a parameter \n",
    "    gtf_exons_gr.Start = gtf_exons_gr.Start-1\n",
    "\n",
    "    # Drop duplicated positions on same strand \n",
    "    gtf_exons_gr = gtf_exons_gr.drop_duplicate_positions(strand=True) # Why are so many gone after this? \n",
    "\n",
    "    # Print the number of unique exons, transcript ids, and gene ids\n",
    "    print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"The number of unique exons is \" + str(len(gtf_exons_gr.exon_id.unique())))\n",
    "    print(\"The number of unique transcript ids is \" + str(len(gtf_exons_gr.transcript_id.unique())))\n",
    "    print(\"The number of unique gene ids is \" + str(len(gtf_exons_gr.gene_id.unique())))\n",
    "    print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    return(gtf_exons_gr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset):\n",
    "    # Assuming 'score' is a column in 'dataset' that you want to summarize\n",
    "    juncs_dat_summ = dataset.groupby([\"chrom\", \"chromStart\", \"chromEnd\", \"junction_id\"], as_index=False).score.sum()\n",
    "    juncs_dat_summ = juncs_dat_summ.merge(\n",
    "        juncs_dat_summ.groupby(['chromStart'])['score'].sum().reset_index().rename(columns={'score': 'total_5SS_counts'}),\n",
    "        on='chromStart'\n",
    "    ).merge(\n",
    "        juncs_dat_summ.groupby(['chromEnd'])['score'].sum().reset_index().rename(columns={'score': 'total_3SS_counts'}),\n",
    "        on='chromEnd'\n",
    "    )\n",
    "    juncs_dat_summ['5SS_usage'] = juncs_dat_summ['score'] / juncs_dat_summ['total_5SS_counts']\n",
    "    juncs_dat_summ['3SS_usage'] = juncs_dat_summ['score'] / juncs_dat_summ['total_3SS_counts']\n",
    "    return juncs_dat_summ\n",
    "\n",
    "def refine_cluster(cluster, clusters_df, preprocessed_data):\n",
    "    clust_dat = clusters_df[clusters_df.Cluster == cluster]\n",
    "    juncs_dat_all = preprocessed_data[preprocessed_data.junction_id.isin(clust_dat.junction_id)]\n",
    "    ss_score = juncs_dat_all[[\"5SS_usage\", \"3SS_usage\"]].min().min()\n",
    "    junc = juncs_dat_all[(juncs_dat_all[\"5SS_usage\"] == ss_score) | (juncs_dat_all[\"3SS_usage\"] == ss_score)].junction_id.values[0]\n",
    "    return [cluster, junc, ss_score]\n",
    "\n",
    "def refine_clusters(clusters, clusters_df, dataset):\n",
    "    preprocessed_data = preprocess_data(dataset)\n",
    "    all_juncs_scores = []\n",
    "    # start time \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(lambda x: refine_cluster(x, clusters_df, preprocessed_data), clusters))\n",
    "    for result in results:\n",
    "        all_juncs_scores.append(result)\n",
    "\n",
    "    # end time\n",
    "    end_time = time.time()\n",
    "    print(\"Refining clusters took \" + str(round((end_time-start_time), 2)) + \" seconds\")\n",
    "    return all_juncs_scores\n",
    "\n",
    "def filter_junctions_by_shared_splice_sites(df):\n",
    "    # Function to apply to each group (cluster)\n",
    "    def filter_group(group):\n",
    "        # Find duplicated start and end positions within the group\n",
    "        duplicated_starts = group['Start'].duplicated(keep=False)\n",
    "        duplicated_ends = group['End'].duplicated(keep=False)\n",
    "        \n",
    "        # Keep rows where either start or end position is duplicated\n",
    "        return group[duplicated_starts | duplicated_ends]\n",
    "    \n",
    "    # Group by 'Cluster' and apply the filtering function\n",
    "    filtered_df = df.groupby('Cluster').apply(filter_group).reset_index(drop=True)\n",
    "    return filtered_df.Cluster.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_junction_files(junc_files, junc_suffix):\n",
    "    \"\"\"\n",
    "    Read junction files.\n",
    "\n",
    "    Parameters:\n",
    "    - junc_files (list): List of paths to junction files.\n",
    "    - junc_suffix (str): Suffix of junction files.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Concatenated DataFrame of junction files.\n",
    "    \"\"\"\n",
    "    all_juncs_list = []\n",
    "\n",
    "    for junc_path in junc_files:\n",
    "        junc_path = Path(junc_path)\n",
    "        print(f\"Reading in junction files from {junc_path}\")\n",
    "\n",
    "        junc_files_in_path = list(junc_path.rglob(junc_suffix))\n",
    "        if not junc_files_in_path:\n",
    "            print(f\"No junction files found in {junc_path} with suffix {junc_suffix}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"The number of junction files to be processed is {len(junc_files_in_path)}\")\n",
    "\n",
    "        files_not_read = []\n",
    "\n",
    "        for junc_file in tqdm(junc_files_in_path):\n",
    "            try:\n",
    "                juncs = pd.read_csv(junc_file, sep=\"\\t\", header=None)\n",
    "                juncs['file_name'] = junc_file  # Add the file name as a new column\n",
    "                juncs['cell_type'] = junc_file\n",
    "                all_juncs_list.append(juncs)  # Append the DataFrame to the list\n",
    "            except Exception as e:\n",
    "                print(f\"Could not read in {junc_file}: {e}\")\n",
    "                files_not_read.append(junc_file)\n",
    "\n",
    "    if len(files_not_read) > 0:\n",
    "        print(\"The total number of files that could not be read is \" + str(len(files_not_read)) + \" as these had no junctions\")\n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    all_juncs = pd.concat(all_juncs_list, ignore_index=True) if all_juncs_list else pd.DataFrame()\n",
    "\n",
    "    return all_juncs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# junc_files defines a path for where junction files can be found, in this case, the path is defined above\n",
    "junc_files = juncs_path\n",
    "\n",
    "# we provide a gtf file for the human genome as well to make better sense of the junctions that are detected in cells\n",
    "# please replace with the path to the gtf file on your system\n",
    "gtf_file=\"/gpfs/commons/groups/knowles_lab/Karin/genome_files/gencode.v43.basic.annotation.gtf\" \n",
    "\n",
    "# define additional parameters \n",
    "sequencing_type = \"single_cell\"\n",
    "\n",
    "# ensure output files are to be saved in output_path \n",
    "output_file = output_path + \"test_intron_clusters\"\n",
    "junc_bed_file= output_path + \"test_juncs.bed\" # you can load this file into IGV to visualize the junction coordinates \n",
    "min_intron_length = 50\n",
    "max_intron_length = 500000\n",
    "threshold_inc = 0.05 \n",
    "min_junc_reads = 2\n",
    "min_num_cells_wjunc = 2\n",
    "keep_singletons = False # ignore junctions that do not share splice sites with any other junction (likely const)\n",
    "junc_suffix = \"*.juncswbarcodes\" \n",
    "\n",
    "if \",\" in junc_files:\n",
    "    junc_files = junc_files.split(\",\")\n",
    "else:\n",
    "    junc_files = [junc_files]\n",
    "all_juncs_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "junc_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gtf_file is not None:\n",
    "    gtf_exons_gr = process_gtf(gtf_file)\n",
    "    print(\"Done extracting exons from gtf file\")\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_juncs = read_junction_files(junc_files, junc_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column names based on sequencing type\n",
    "col_names = [\"chrom\", \"chromStart\", \"chromEnd\", \"name\", \"score\", \"strand\", \n",
    "         \"thickStart\", \"thickEnd\", \"itemRgb\", \"blockCount\", \"blockSizes\", \"blockStarts\"]\n",
    "if sequencing_type == \"single_cell\":\n",
    "    col_names += [\"num_cells_wjunc\", \"cell_readcounts\"]\n",
    "col_names += [\"file_name\", \"cell_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_juncs(all_juncs, col_names, min_intron, max_intron):\n",
    "    \n",
    "    # Apply column names to the DataFrame\n",
    "    all_juncs.columns = col_names\n",
    "    \n",
    "    # Split 'blockSizes' into two new columns and convert them to integers (this step takes a while)\n",
    "    all_juncs[['block_add_start', 'block_subtract_end']] = all_juncs[\"blockSizes\"].str.split(',', expand=True).astype(int)\n",
    "\n",
    "    # Adjust 'chromStart' and 'chromEnd' based on 'block_add_start' and 'block_subtract_end'\n",
    "    all_juncs[\"chromStart\"] += all_juncs['block_add_start']\n",
    "    all_juncs[\"chromEnd\"] -= all_juncs['block_subtract_end']\n",
    "\n",
    "    # Calculate 'intron_length' and filter based on 'min_intron' and 'max_intron'\n",
    "    all_juncs[\"intron_length\"] = all_juncs[\"chromEnd\"] - all_juncs[\"chromStart\"]\n",
    "    mask = (all_juncs[\"intron_length\"] >= min_intron) & (all_juncs[\"intron_length\"] <= max_intron)\n",
    "    all_juncs = all_juncs[mask]\n",
    "\n",
    "    # Filter for 'chrom' column to handle \"chr\" prefix\n",
    "    all_juncs = all_juncs.copy()\n",
    "\n",
    "    # New filter for 'chrom' column to handle \"chr\" prefix, using .loc for safe in-place modification\n",
    "    standard_chromosomes_pattern = r'^(?:chr)?(?:[1-9]|1[0-9]|2[0-2]|X|Y|MT)$'\n",
    "    all_juncs = all_juncs[all_juncs['chrom'].str.match(standard_chromosomes_pattern)]\n",
    "\n",
    "    print(\"Cleaning up 'chrom' column\")\n",
    "    # Remove \"chr\" prefix from 'chrom' column\n",
    "    all_juncs['chrom'] = all_juncs['chrom'].str.replace(r'^chr', '', regex=True)\n",
    "    \n",
    "    # Add 'junction_id' column\n",
    "    all_juncs['junction_id'] = all_juncs['chrom'] + '_' + all_juncs['chromStart'].astype(str) + '_' + all_juncs['chromEnd'].astype(str)\n",
    "    \n",
    "    # Get total score for each junction and merge with all_juncs with new column \"total_counts\"\n",
    "    all_juncs = all_juncs.groupby('junction_id').agg({'score': 'sum'}).reset_index().merge(all_juncs, on='junction_id', how='left')\n",
    "\n",
    "    # rename score_x and score_y to total_junc_counts and score \n",
    "    all_juncs.rename(columns={'score_x': 'counts_total', 'score_y': 'score'}, inplace=True)\n",
    "\n",
    "    return(all_juncs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_juncs = clean_up_juncs(all_juncs, col_names, min_intron_length, max_intron_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Make gr object from ALL junctions across all cell types \n",
    "print(\"Making gr object from all junctions across all cell types\")\n",
    "juncs_gr = pr.from_dict({\"Chromosome\": all_juncs[\"chrom\"], \"Start\": all_juncs[\"chromStart\"], \"End\": all_juncs[\"chromEnd\"], \"Strand\": all_juncs[\"strand\"], \"Cell\": all_juncs[\"cell_type\"], \"junction_id\": all_juncs[\"junction_id\"], \"counts_total\": all_juncs[\"counts_total\"]})\n",
    "juncs_gr = juncs_gr[[\"Chromosome\", \"Start\", \"End\", \"Strand\", \"junction_id\", \"counts_total\"]].drop_duplicate_positions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if min_junc_reads is not none then remove junctions with less than min_junc_reads\n",
    "if min_junc_reads is not None:\n",
    "    juncs_gr = juncs_gr[juncs_gr.counts_total > min_junc_reads]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping_juncs_exons(juncs_gr, gtf_exons_gr, singletons):\n",
    "    print(\"Annotating junctions with known exons based on input gtf file\")\n",
    "    \n",
    "    # for each junction, the start of the junction should equal end of exons and end of junction should equal start of exon \n",
    "    juncs_gr = juncs_gr.k_nearest(gtf_exons_gr, strandedness = \"same\", ties=\"different\", k=2, overlap=False)\n",
    "    # ensure distance parameter is still 1 \n",
    "    juncs_gr = juncs_gr[abs(juncs_gr.Distance) == 1]\n",
    "\n",
    "    # group juncs_gr by gene_id and ensure that each junction has Start and End aligning with at least one End_b and Start_b respectively\n",
    "    grouped_gr = juncs_gr.df.groupby(\"gene_id\")\n",
    "    juncs_keep = []\n",
    "    for name, group in grouped_gr:\n",
    "        group = group[(group.Start.isin(group.End_b)) & (group.End.isin(group.Start_b))]\n",
    "        # save junctions that are found here after filtering for matching start and end positions\n",
    "        juncs_keep.append(group.junction_id.unique())\n",
    "\n",
    "    # flatten the list of lists\n",
    "    juncs_keep = [item for sublist in juncs_keep for item in sublist]\n",
    "    juncs_gr = juncs_gr[juncs_gr.junction_id.isin(juncs_keep)]\n",
    "    \n",
    "    print(\"The number of junctions after assessing distance to exons is \" + str(len(juncs_gr.junction_id.unique())))\n",
    "    if len(juncs_gr.junction_id.unique()) < 5000:\n",
    "        print(\"There are less than 5000 junctions after assessing distance to exons. Please check your gtf file and ensure that it is in the correct format (start and end positions are not off by 1).\", flush=True)\n",
    "    \n",
    "    print(\"Clustering intron splicing events by gene_id\")\n",
    "    juncs_coords_unique = juncs_gr[['Chromosome', 'Start', 'End', 'Strand', 'junction_id', 'gene_id']].drop_duplicate_positions()\n",
    "    clusters = juncs_coords_unique.cluster(by=\"gene_id\", slack=-1, count=True)\n",
    "    print(\"The number of clusters after removing singletons is \" + str(len(clusters.Cluster.unique())))\n",
    "\n",
    "    if singletons == False:\n",
    "        # remove singletons \n",
    "        clusters = clusters[clusters.Count > 1]\n",
    "        print(\"The number of clusters after removing singletons is \" + str(len(clusters.Cluster.unique())))\n",
    "        # update juncs_gr to only include junctions that are part of clusters\n",
    "        juncs_gr = juncs_gr[juncs_gr.junction_id.isin(clusters.junction_id)]\n",
    "        # update juncs_coords_unique to only include junctions that are part of clusters\n",
    "        juncs_coords_unique = juncs_coords_unique[juncs_coords_unique.junction_id.isin(clusters.junction_id)]\n",
    "        print(\"The number of junctions after removing singletons is \" + str(len(juncs_coords_unique.junction_id.unique())))\n",
    "        return juncs_gr, juncs_coords_unique, clusters\n",
    "    else:\n",
    "        return juncs_gr, juncs_coords_unique, clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gtf_file is not None:\n",
    "    juncs_gr, juncs_coords_unique, clusters = mapping_juncs_exons(juncs_gr, gtf_exons_gr, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now for each cluster we want to check that each junction shares a splice site with at least one other junction in the cluster\n",
    "clusts_keep = filter_junctions_by_shared_splice_sites(clusters.df)\n",
    "# update clusters, juncs_gr, and juncs_coords_unique to only include clusters\n",
    "clusters = clusters[clusters.Cluster.isin(clusts_keep)]\n",
    "juncs_gr = juncs_gr[juncs_gr.junction_id.isin(clusters.junction_id)]\n",
    "juncs_coords_unique = juncs_coords_unique[juncs_coords_unique.junction_id.isin(clusters.junction_id)]\n",
    "print(\"The number of clusters after filtering for shared splice sites is \" + str(len(clusters.Cluster.unique())))\n",
    "print(\"The number of junctions after filtering for shared splice sites is \" + str(len(juncs_coords_unique.junction_id.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update our all_juncs file to only include junctions that are part of clusters\n",
    "all_juncs = all_juncs[all_juncs.junction_id.isin(juncs_coords_unique.junction_id)]\n",
    "# now ensure that each junction in each cluster is refined to the junction with the lowest 5SS_usage or 3SS_usage\n",
    "all_juncs_scores = refine_clusters(clusters.Cluster.unique(), clusters, all_juncs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "junc_scores_all = pd.DataFrame(all_juncs_scores, columns=[\"Cluster\", \"junction_id\", \"junction_score\"])\n",
    "junc_scores_all = junc_scores_all[junc_scores_all.junction_score < threshold_inc]\n",
    "juncs_gr = juncs_gr[~juncs_gr.junction_id.isin(junc_scores_all.junction_id)]\n",
    "clusters = clusters[~clusters.junction_id.isin(junc_scores_all.junction_id)]\n",
    "all_juncs = all_juncs[~all_juncs.junction_id.isin(junc_scores_all.junction_id)]\n",
    "juncs_coords_unique = juncs_coords_unique[~juncs_coords_unique.junction_id.isin(junc_scores_all.junction_id)]\n",
    "print(\"The number of clusters after removing low confidence junctions is \" + str(len(clusters.Cluster.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juncs_gr = juncs_gr[~juncs_gr.junction_id.isin(junc_scores_all.junction_id)]\n",
    "juncs_gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juncs_gr = juncs_gr.drop_duplicate_positions()\n",
    "clusters = juncs_gr.cluster(by=\"gene_id\", slack=-1, count=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = clusters[clusters.Count > 1]\n",
    "\n",
    "# update juncs_gr to only include junctions that are part of clusters and update juncs_coords_unique to only include junctions that are part of clusters\n",
    "juncs_gr = juncs_gr[juncs_gr.junction_id.isin(clusters.junction_id)]\n",
    "juncs_coords_unique = juncs_coords_unique[juncs_coords_unique.junction_id.isin(clusters.junction_id)]\n",
    "print(\"The number of clusters after removing singletons is \" + str(len(clusters.Cluster.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_juncs = all_juncs[all_juncs.junction_id.isin(juncs_coords_unique.junction_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. After re-clustering above, need to confirm that junctions still share splice sites  \n",
    "print(\"Confirming that junctions in each cluster share splice sites\")\n",
    "clusts_keep = filter_junctions_by_shared_splice_sites(clusters.df)\n",
    "# update clusters, juncs_gr, and juncs_coords_unique to only include clusters\n",
    "clusters = clusters[clusters.Cluster.isin(clusts_keep)]\n",
    "juncs_gr = juncs_gr[juncs_gr.junction_id.isin(clusters.junction_id)]\n",
    "juncs_coords_unique = juncs_coords_unique[juncs_coords_unique.junction_id.isin(clusters.junction_id)]\n",
    "print(\"The number of clusters after filtering for shared splice sites is \" + str(len(clusters.Cluster.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusts_unique = clusters.df[[\"Cluster\", \"junction_id\", \"gene_id\", \"Count\"]].drop_duplicates()\n",
    "clusts_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_juncs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_juncs = all_juncs[all_juncs.junction_id.isin(juncs_coords_unique.junction_id)]\n",
    "all_juncs_df = all_juncs.merge(clusts_unique, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juncs_gr = juncs_gr[[\"Chromosome\", \"Start\", \"End\", \"Strand\", \"junction_id\"]]\n",
    "juncs_gr = juncs_gr.drop_duplicate_positions()\n",
    "juncs_gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(all_juncs_df.junction_id.unique()) == len(clusters.df.junction_id.unique())\n",
    "assert len(all_juncs_df.Cluster.unique()) == len(clusters.df.Cluster.unique()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_juncs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_junctions(dat, junc_id):\n",
    "    # Filter data for the specific junction ID\n",
    "    dat = dat[dat.Cluster == dat[dat.junction_id == junc_id].Cluster.values[0]]\n",
    "\n",
    "    # Get junctions\n",
    "    juncs = dat[[\"chromStart\", \"chromEnd\", \"strand\"]]\n",
    "    juncs = juncs.drop_duplicates()\n",
    "\n",
    "    # Sort junctions based on strand\n",
    "    if juncs.strand.values[0] == \"+\":\n",
    "        juncs = juncs.sort_values(\"chromStart\")\n",
    "    else:\n",
    "        juncs = juncs.sort_values(\"chromEnd\", ascending=False)\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(10, len(juncs) * 0.5))\n",
    "\n",
    "    # Plot junctions as lines\n",
    "    for i, (_, junc) in enumerate(juncs.iterrows()):\n",
    "        ax.plot([junc[\"chromStart\"], junc[\"chromEnd\"]], [i, i], color=\"red\")\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_xlabel(\"Genomic Position\")\n",
    "    ax.set_yticks(list(range(len(juncs))))\n",
    "    ax.set_title(f\"Visualization of Junctions in Cluster {dat.Cluster.values[0]} in the Gene {dat.gene_id.values[0]}\")\n",
    "    print(\"The junction of interest is \" + junc_id)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = all_juncs_df.junction_id.sample(1).values[0]\n",
    "visualize_junctions(all_juncs_df, j)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeafletSC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
