{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch \n",
    "# still do preprocessing in scipy\n",
    "import scipy.sparse as sp\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# import factor model from beta-dirichlet-factor\n",
    "sys.path.append('/gpfs/commons/home/kisaev/Leaflet/src/beta-dirichlet-factor')\n",
    "import factor_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaflet_repo = '/gpfs/commons/home/kisaev/Leaflet/src/beta-binomial-mix/'\n",
    "sys.path.append(leaflet_repo)\n",
    "import load_cluster_data\n",
    "from betabinomo_mix_singlecells import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings and Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "float_type = { \n",
    "    \"device\" : device, \n",
    "    \"dtype\" : torch.float, # save memory\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files_folder = '/gpfs/commons/groups/knowles_lab/Karin/Leaflet-analysis-WD/TabulaMurisBrain/MLCB_Brain_true/FULL/'\n",
    "cell_types = ['Brain_Non-Myeloid_brain_pericyte',\n",
    " 'Brain_Non-Myeloid_oligodendrocyte_precursor_cell',\n",
    " 'Brain_Non-Myeloid_endothelial_cell' 'Brain_Non-Myeloid_oligodendrocyte',\n",
    " 'Brain_Non-Myeloid_neuron', 'Brain_Myeloid_macrophage',\n",
    " 'Brain_Myeloid_microglial_cell', 'Brain_Non-Myeloid_astrocyte']#ignoring Bergmann_glial_cell since very few cells\n",
    "\n",
    "# convert data to Leaflet required input formats \n",
    "final_data, coo_counts_sparse, coo_cluster_sparse, cell_ids_conversion, junction_ids_conversion = load_cluster_data.load_cluster_data(\n",
    "    input_folder = input_files_folder, max_intron_count=5000, celltypes=cell_types, has_genes=\"yes\") \n",
    "\n",
    "# add cluster to final_data \n",
    "final_data = final_data.merge(junction_ids_conversion, on=[\"junction_id_index\"], how=\"left\")\n",
    "\n",
    "# get indices (maybe don't need this actually)\n",
    "indices = (final_data.cell_id_index, final_data.junction_id_index)\n",
    "indices_np = np.stack(indices)\n",
    "junc_counts = sp.coo_matrix((final_data.junc_count, indices))\n",
    "cluster_counts = sp.coo_matrix((final_data.cluster_count, indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_index_tensor, junc_index_tensor, my_data = make_torch_data(final_data, **float_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.juncratio.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_data = final_data[[\"cell_id_index\", \"Cluster\", \"cell_type\", \"junction_id_index\", \"juncratio\", \"junc_count\", \"cluster_count\",  \"junction_id\", \"gene_id\"]]\n",
    "#sns.kdeplot(data=final_data, x=\"juncratio\", hue=\"cell_type\", cumulative=True, common_norm=False, common_grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start with a simple PCA using imputed values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make matrix of junctions by cells and fill with juncratio values \n",
    "mat = np.zeros((len(final_data.cell_id_index.unique()), len(final_data.junction_id_index.unique())))\n",
    "mat[final_data.cell_id_index, final_data.junction_id_index] = final_data.juncratio\n",
    "mat[~(final_data.cell_id_index), ~(final_data.junction_id_index)] = np.nan\n",
    "\n",
    "# imput nan values using mean of each row (does this make sense?)\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(mat)\n",
    "mat_imputed = imp.transform(mat)\n",
    "\n",
    "# run PCA on mat_imputed \n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(mat_imputed)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs = pca.transform(mat_imputed)\n",
    "pcs_df = pd.DataFrame(pcs, columns=[\"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\", \"PC6\", \"PC7\", \"PC8\", \"PC9\", \"PC10\"])\n",
    "cell_types_pcs = cell_ids_conversion[\"cell_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot first two PCs\n",
    "sns.scatterplot(data=pcs_df, x=\"PC1\", y=\"PC2\", hue=cell_types_pcs.values, s=5, alpha=0.5)\n",
    "# move legend outside of plot\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"PCA of Junction Usage Ratios (imputed matrix)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot first and third PCs\n",
    "sns.scatterplot(data=pcs_df, x=\"PC2\", y=\"PC3\", hue=cell_types_pcs.values, s=5, alpha=0.5)\n",
    "# move legend outside of plot\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"PCA of Junction Usage Ratios (imputed matrix)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a quick look at coverage of reads across junctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_junc_cov = final_data.groupby(\"cell_id_index\").junc_count.sum()\n",
    "cell_cluster_cov = final_data.groupby(\"cell_id_index\").cluster_count.sum()\n",
    "\n",
    "# plot histogram of coverage\n",
    "sns.histplot(cell_junc_cov/1000)\n",
    "plt.title(\"Junction Coverage per Cell (read counts/1000)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram of coverage clusters \n",
    "sns.histplot(cell_cluster_cov/1000)\n",
    "plt.title(\"Cluster Coverage per Cell (read counts/1000)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get ready for model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == torch.device('cuda'):\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = torch.tensor(final_data[['cell_id_index', 'junction_id_index']].to_numpy().T, dtype=torch.long)\n",
    "values = torch.tensor(final_data['junc_count'].to_numpy(), dtype=torch.float)\n",
    "# Determine the size of the tensor\n",
    "num_cells = final_data['cell_id_index'].max() + 1\n",
    "num_junctions = final_data['junction_id_index'].max() + 1\n",
    "size = (num_cells, num_junctions)\n",
    "# Create a sparse tensor\n",
    "y_tensor = torch.sparse_coo_tensor(indices, values, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_counts = torch.tensor(final_data[['cell_id_index', 'junction_id_index']].to_numpy().T, dtype=torch.long)\n",
    "values_counts = torch.tensor(final_data['cluster_count'].to_numpy(), dtype=torch.float)\n",
    "# Determine the size of the tensor\n",
    "num_cells_counts = final_data['cell_id_index'].max() + 1\n",
    "num_junctions_counts = final_data['junction_id_index'].max() + 1\n",
    "size_counts = (num_cells_counts, num_junctions_counts)\n",
    "# Create a sparse tensor\n",
    "total_counts_tensor = torch.sparse_coo_tensor(indices_counts, values_counts, size_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run factor model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import reload \n",
    "from importlib import reload\n",
    "reload(factor_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 6\n",
    "losses, sampled_guide, latent_vars = factor_model.main(y_tensor, total_counts_tensor, use_global_prior=True, K=K, lr=0.01, loss_plot=True, num_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract latent variables\n",
    "pi = latent_vars[\"pi\"] # overall contribution of each factor to cell population\n",
    "conc = latent_vars[\"conc\"] # one scaling value \n",
    "\n",
    "assign_post = latent_vars[\"assign\"]\n",
    "\n",
    "psis = latent_vars[\"psi\"] # psi is the probability of a junction being used in a cluster\n",
    "a = latent_vars[\"a\"] # a is the alpha parameter of the beta distribution (if no global prior used, then this is one value per junction)\n",
    "b = latent_vars[\"b\"] # b is the beta parameter of the beta distribution (if no global prior used, then this is one value per junction)\n",
    "\n",
    "#a_global = latent_vars[\"a_global\"] # a_global is the alpha parameter of the beta distribution (if global prior used, then this is one value for all junctions)\n",
    "#b_global = latent_vars[\"b_global\"] # b_global is the beta parameter of the beta distribution (if global prior used, then this is one value for all junctions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The inferred concentration parameter is: \" + str(conc))\n",
    "print(\"The inferred pi parameter is: \" + str(pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate latent variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_palette = sns.color_palette(\"Set1\", n_colors=len(cell_ids_conversion['cell_type'].unique()))\n",
    "# Create a color bar legend\n",
    "legend = sns.color_palette(palette=color_palette, as_cmap=True)\n",
    "\n",
    "# Obtain cell type labels for every cell in the matrix also \n",
    "unique_cell_types = cell_ids_conversion['cell_type'].unique()\n",
    "num_unique_types = len(unique_cell_types)\n",
    "colors = sns.color_palette('Set1', n_colors=num_unique_types)  # You can use any color palette\n",
    "cell_types = cell_ids_conversion.cell_type.values\n",
    "\n",
    "# create colours for each cell type \n",
    "cell_type_colors = {cell_type: color for cell_type, color in zip(unique_cell_types, colors)}\n",
    "row_colors = [cell_type_colors[cell_type] for cell_type in cell_types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_post.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = sns.clustermap(\n",
    "    data=assign_post,\n",
    "    method='complete',\n",
    "    cmap=\"viridis\",\n",
    "    annot=False,\n",
    "    fmt=\".2f\",\n",
    "    yticklabels=False,\n",
    "    figsize=(8, 8),\n",
    "    center=0,\n",
    "    row_colors=row_colors,  # Apply row colors\n",
    "    cbar_kws={'label': 'Post assignment'} \n",
    "    )\n",
    "cluster.cax.set_ylabel('Post assignment', size=8)\n",
    "# Increase font size for color bar tick labels:\n",
    "cbar_ax = cluster.cax\n",
    "for label in cbar_ax.yaxis.get_ticklabels():\n",
    "    label.set_size(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print cell type color legend seperately\n",
    "cell_type_colors\n",
    "# Create a color bar legend\n",
    "legend = sns.color_palette(palette=color_palette, as_cmap=True)\n",
    "sns.palplot(color_palette)\n",
    "plt.title(\"Cell Type Legend\")\n",
    "# add cell type names to legend\n",
    "plt.xticks(np.arange(len(unique_cell_types)), unique_cell_types, rotation=45, ha='right')\n",
    "plt.xlabel(\"Cell Type\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's calculate silhouette score ussing assign_post\n",
    "ss = silhouette_score(assign_post, cell_types)\n",
    "# print with K equals K the ss is \n",
    "print(\"The silhouette score for K = \" + str(K) + \" is: \" + str(ss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "junc_probs = a / (a + b)\n",
    "junc_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_psis = a_global / (a_global + b_global)\n",
    "global_psis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(global_psis, cumulative=True)\n",
    "# add title: global junction PSIs (global a / global a + global b)\n",
    "plt.title(\"Global Junction PSIs\")\n",
    "# draw a dashed red line at 0.5\n",
    "plt.axhline(y=0.5, color='red', linestyle='--')\n",
    "# xlab add PSI \n",
    "plt.xlabel(\"Global PSI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot junction usage across factors using heatmap\n",
    "# let's just look at factor 1 \n",
    "factor = 1\n",
    "# get junction usage for factor 1\n",
    "factor_junc_usage = psis[1:4, :]\n",
    "factor_junc_usage\n",
    "# make heatmap to visualize junction usage across factors\n",
    "sns.clustermap(factor_junc_usage, cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make histogram from np array to visualize distribution of probabilities for junction usages across factors \n",
    "# Number of factors (columns)\n",
    "num_factors = psis.shape[0]\n",
    "\n",
    "# Plotting KDE cumulative plot for each column\n",
    "for i in range(num_factors):\n",
    "    sns.kdeplot(psis[i, :], cumulative=True, label=f'Factor {i+1}')\n",
    "\n",
    "plt.title('Latent junction PSIs across factors')\n",
    "plt.xlabel('Junction-Factor PSI')\n",
    "plt.ylabel('Cumulative Density')\n",
    "plt.axhline(y=0.5, color='red', linestyle='--')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leafcutter-sc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
