{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from load_cluster_data import load_cluster_data\n",
    "from pca_kmeans_init import pca_kmeans_init\n",
    "from betabinomo_mix_singlecells import *\n",
    "import betabinomo_mix_singlecells\n",
    "reload(betabinomo_mix_singlecells)\n",
    "import torch\n",
    "import sklearn.manifold \n",
    "import plotnine as p9\n",
    "import time\n",
    "# indicate plot should be small 4 by 4\n",
    "import plotnine as p9\n",
    "from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap, geom_violin, theme, element_blank, geom_text\n",
    "import plotnine\n",
    "from tqdm import tqdm\n",
    "plotnine.options.figure_size = (4, 4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings and Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "input_file = '/gpfs/commons/scratch/kisaev/ss_tabulamuris_test/Leaflet/clustered_junctions_noanno.txt_anno_free_50_500000_10_5_0.1_single_cell.h5'\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "MAKE_PCA_TSNE = True\n",
    "\n",
    "float_type = { \n",
    "    \"device\" : device, \n",
    "    \"dtype\" : torch.float, # save memory\n",
    "}\n",
    "\n",
    "hypers = {\n",
    "    \"eta\" : 1., \n",
    "    \"alpha_prior\" : 1., # karin had 0.65 \n",
    "    \"pi_prior\" : 1.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Brain_Myeloid_macrophage_pseudobulk'\n",
      " 'Brain_Non-Myeloid_Bergmann_glial_cell_pseudobulk'\n",
      " 'Brain_Non-Myeloid_astrocyte_pseudobulk'\n",
      " 'Brain_Non-Myeloid_brain_pericyte_pseudobulk'\n",
      " 'Brain_Non-Myeloid_endothelial_cell_pseudobulk'\n",
      " 'Brain_Non-Myeloid_neuron_pseudobulk'\n",
      " 'Brain_Non-Myeloid_oligodendrocyte_precursor_cell_pseudobulk'\n",
      " 'Brain_Non-Myeloid_oligodendrocyte_pseudobulk']\n",
      "3462\n",
      "3462\n",
      "63257\n",
      "63257\n",
      "The number of junctions in the data is:  63257\n",
      "The number of cells in the data is:  3462\n",
      "The number of cell types in the data is:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/commons/home/kisaev/leafcutter-sc/src/beta-binomial-lda/betabinomo_mix_singlecells.py:299: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n"
     ]
    }
   ],
   "source": [
    "final_data, coo_counts_sparse, coo_cluster_sparse, cell_ids_conversion, junction_ids_conversion = load_cluster_data(\n",
    "    input_file) \n",
    "N = coo_cluster_sparse.shape[0]\n",
    "J = coo_cluster_sparse.shape[1]\n",
    "\n",
    "cell_index_tensor, junc_index_tensor, my_data = betabinomo_mix_singlecells.make_torch_data(final_data, **float_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize VI params\n",
      "Got the initial ELBO ^\n",
      "ELBO converged @ -1678499072.0  CAVI iteration #  18  complete\n",
      "Initialize VI params\n",
      "Got the initial ELBO ^\n",
      "ELBO converged @ -1678051968.0  CAVI iteration #  30  complete\n",
      "This took 15.786423921585083 seconds\n"
     ]
    }
   ],
   "source": [
    "num_trials = 2 # should also be an argument that gets fed in\n",
    "num_iters = 30 # should also be an argument that gets fed in\n",
    "K = 5\n",
    "\n",
    "# loop over the number of trials (for now just testing using one trial but in general need to evaluate how performance is affected by number of trials)\n",
    "reload(betabinomo_mix_singlecells)\n",
    "\n",
    "start_time = time.time()\n",
    "results = [ betabinomo_mix_singlecells.calculate_CAVI(K, my_data, float_type, hypers, init_labels = None, num_iterations = num_iters) \n",
    "           for t in range(num_trials) ]\n",
    "\n",
    "# write the above line use fstring\n",
    "print(f\"This took {time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cell_id  cluster_id  iteration\n",
      "0        0           3          0\n",
      "1        1           3          0\n",
      "2        2           3          0\n",
      "3        3           3          0\n",
      "4        4           3          0\n"
     ]
    }
   ],
   "source": [
    "# extract PHI_f from every trial in num_trials\n",
    "all_iters_PHI_f = [ result[3] for result in results ]\n",
    "i = 0\n",
    "\n",
    "# Create an empty list to store DataFrames from each iteration\n",
    "dfs_list = []\n",
    "\n",
    "for PHI_var in all_iters_PHI_f:\n",
    "\n",
    "    probability_tensor = PHI_var\n",
    "\n",
    "    # Create an array with cell IDs (e.g., cell_0, cell_1, ..., cell_(N-1))\n",
    "    cell_ids = np.arange(probability_tensor.shape[0])\n",
    "    cell_ids = [cell_id for cell_id in cell_ids]\n",
    "\n",
    "    # Get the cluster IDs for each cell based on the maximum probability\n",
    "    cluster_ids = np.argmax(probability_tensor, axis=1)\n",
    "\n",
    "    # Create a DataFrame with the cell_id, cluster_id, and probability columns\n",
    "    df = pd.DataFrame({\"cell_id\": cell_ids, \"cluster_id\": cluster_ids})\n",
    "\n",
    "    # Add column with iteration number\n",
    "    df[\"iteration\"] = i\n",
    "    i += 1\n",
    "    # Append the DataFrame to the list\n",
    "    dfs_list.append(df)\n",
    "\n",
    "# Concatenate all the DataFrames into a single DataFrame\n",
    "concatenated_df = pd.concat(dfs_list, ignore_index=True)\n",
    "\n",
    "# Display the concatenated DataFrame\n",
    "print(concatenated_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "all_cell_pairs = list(combinations(concatenated_df.cell_id.unique(), 2))\n",
    "grouped_data = concatenated_df.groupby(['cell_id', 'iteration'])['cluster_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pairs:   7%|â–‹         | 402402/5990991 [01:36<22:15, 4183.31it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[116], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m cluster_ids_cell_2 \u001b[39m=\u001b[39m unique_clusters_by_cell[cell_2]\n\u001b[1;32m     24\u001b[0m \u001b[39m# Check if the cluster IDs are the same in both cells' iterations\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m is_same_cluster \u001b[39m=\u001b[39m (cluster_ids_cell_1 \u001b[39m==\u001b[39;49m cluster_ids_cell_2)\u001b[39m.\u001b[39mall()\n\u001b[1;32m     27\u001b[0m \u001b[39m# Append the result to the list\u001b[39;00m\n\u001b[1;32m     28\u001b[0m results\u001b[39m.\u001b[39mappend(((cell_1, cell_2), is_same_cluster))\n",
      "File \u001b[0;32m~/miniconda3/envs/leafcutter-sc/lib/python3.9/site-packages/pandas/core/ops/common.py:65\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     \u001b[39mfor\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m [ABCDataFrame, ABCSeries, ABCIndex]:\n\u001b[0;32m---> 65\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39mcls\u001b[39;49m):\n\u001b[1;32m     66\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     67\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(other, \u001b[39mcls\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/leafcutter-sc/lib/python3.9/site-packages/pandas/core/dtypes/generic.py:45\u001b[0m, in \u001b[0;36mcreate_pandas_abc_type.<locals>._instancecheck\u001b[0;34m(cls, inst)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(inst, attr, \u001b[39m\"\u001b[39m\u001b[39m_typ\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39min\u001b[39;00m comp\n\u001b[1;32m     43\u001b[0m \u001b[39m# https://github.com/python/mypy/issues/1006\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39m# error: 'classmethod' used with a non-method\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[39m@classmethod\u001b[39m  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_instancecheck\u001b[39m(\u001b[39mcls\u001b[39m, inst) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[1;32m     47\u001b[0m     \u001b[39mreturn\u001b[39;00m _check(inst) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(inst, \u001b[39mtype\u001b[39m)\n\u001b[1;32m     49\u001b[0m \u001b[39m@classmethod\u001b[39m  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_subclasscheck\u001b[39m(\u001b[39mcls\u001b[39m, inst) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[1;32m     51\u001b[0m     \u001b[39m# Raise instead of returning False\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[39m# This is consistent with default __subclasscheck__ behavior\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "\n",
    "# Assuming you have a DataFrame called 'concatenated_df' with 'cell_id', 'iteration', and 'cluster_id' columns\n",
    "\n",
    "# Preprocess the DataFrame to get the unique cluster IDs for each 'cell_id' and 'iteration'\n",
    "preprocessed_data = concatenated_df.groupby(['cell_id', 'iteration'])['cluster_id'].unique()\n",
    "\n",
    "# Create a dictionary to store the unique cluster IDs for each cell\n",
    "unique_clusters_by_cell = {cell: preprocessed_data[cell] for cell in preprocessed_data.index.get_level_values('cell_id').unique()}\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Loop through all the cell pairs in 'all_cell_pairs'\n",
    "for cell_1, cell_2 in tqdm(all_cell_pairs, desc=\"Processing pairs\"):\n",
    "    # Check if both cells have iterations\n",
    "    if cell_1 in unique_clusters_by_cell and cell_2 in unique_clusters_by_cell:\n",
    "        # Get the unique cluster IDs for both cells\n",
    "        cluster_ids_cell_1 = unique_clusters_by_cell[cell_1]\n",
    "        cluster_ids_cell_2 = unique_clusters_by_cell[cell_2]\n",
    "\n",
    "        # Check if the cluster IDs are the same in both cells' iterations\n",
    "        is_same_cluster = (cluster_ids_cell_1 == cluster_ids_cell_2).all()\n",
    "\n",
    "        # Append the result to the list\n",
    "        results.append(((cell_1, cell_2), is_same_cluster))\n",
    "\n",
    "# Display the results\n",
    "for cell_pair, is_same_cluster in results:\n",
    "    print(f\"Cell pair: {cell_pair}, Same cluster assignments: {is_same_cluster}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3462 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/3462 [01:01<6:30:24,  6.78s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39m# for each subpair in cell_pairs, check if they get assigned together across iterations in concatenated_df\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m subpair \u001b[39min\u001b[39;00m cell_pairs:\n\u001b[1;32m      9\u001b[0m     \u001b[39m# get data \u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     test\u001b[39m=\u001b[39mconcatenated_df[concatenated_df[\u001b[39m\"\u001b[39;49m\u001b[39mcell_id\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49misin([subpair[\u001b[39m0\u001b[39;49m], subpair[\u001b[39m1\u001b[39;49m]])]\u001b[39m.\u001b[39mgroupby([\u001b[39m\"\u001b[39m\u001b[39miteration\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mcluster_id\u001b[39m.\u001b[39munique()\n\u001b[1;32m     11\u001b[0m     \u001b[39m# check if in both iterations, the length is 1 (that means they are in the same cluster)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     res \u001b[39m=\u001b[39m test\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39mlen\u001b[39m(x) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munique()\n",
      "File \u001b[0;32m~/miniconda3/envs/leafcutter-sc/lib/python3.9/site-packages/pandas/core/frame.py:3798\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3796\u001b[0m \u001b[39m# Do we have a (boolean) 1d indexer?\u001b[39;00m\n\u001b[1;32m   3797\u001b[0m \u001b[39mif\u001b[39;00m com\u001b[39m.\u001b[39mis_bool_indexer(key):\n\u001b[0;32m-> 3798\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_bool_array(key)\n\u001b[1;32m   3800\u001b[0m \u001b[39m# We are left with two options: a single key, and a collection of keys,\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m \u001b[39m# We interpret tuples as collections only for non-MultiIndex\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m is_single_key \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(key, \u001b[39mtuple\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m is_list_like(key)\n",
      "File \u001b[0;32m~/miniconda3/envs/leafcutter-sc/lib/python3.9/site-packages/pandas/core/frame.py:3853\u001b[0m, in \u001b[0;36mDataFrame._getitem_bool_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3851\u001b[0m key \u001b[39m=\u001b[39m check_bool_indexer(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, key)\n\u001b[1;32m   3852\u001b[0m indexer \u001b[39m=\u001b[39m key\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]\n\u001b[0;32m-> 3853\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take_with_is_copy(indexer, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/leafcutter-sc/lib/python3.9/site-packages/pandas/core/generic.py:3902\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[0;34m(self, indices, axis)\u001b[0m\n\u001b[1;32m   3894\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_take_with_is_copy\u001b[39m(\u001b[39mself\u001b[39m: NDFrameT, indices, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NDFrameT:\n\u001b[1;32m   3895\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3896\u001b[0m \u001b[39m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[1;32m   3897\u001b[0m \u001b[39m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3900\u001b[0m \u001b[39m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[1;32m   3901\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3902\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take(indices\u001b[39m=\u001b[39;49mindices, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[1;32m   3903\u001b[0m     \u001b[39m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[1;32m   3904\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m result\u001b[39m.\u001b[39m_get_axis(axis)\u001b[39m.\u001b[39mequals(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_axis(axis)):\n",
      "File \u001b[0;32m~/miniconda3/envs/leafcutter-sc/lib/python3.9/site-packages/pandas/core/generic.py:3886\u001b[0m, in \u001b[0;36mNDFrame._take\u001b[0;34m(self, indices, axis, convert_indices)\u001b[0m\n\u001b[1;32m   3879\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3880\u001b[0m \u001b[39mInternal version of the `take` allowing specification of additional args.\u001b[39;00m\n\u001b[1;32m   3881\u001b[0m \n\u001b[1;32m   3882\u001b[0m \u001b[39mSee the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[1;32m   3883\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3884\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_consolidate_inplace()\n\u001b[0;32m-> 3886\u001b[0m new_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mtake(\n\u001b[1;32m   3887\u001b[0m     indices,\n\u001b[1;32m   3888\u001b[0m     axis\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_block_manager_axis(axis),\n\u001b[1;32m   3889\u001b[0m     verify\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   3890\u001b[0m     convert_indices\u001b[39m=\u001b[39;49mconvert_indices,\n\u001b[1;32m   3891\u001b[0m )\n\u001b[1;32m   3892\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(new_data)\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtake\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/leafcutter-sc/lib/python3.9/site-packages/pandas/core/internals/managers.py:978\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[0;34m(self, indexer, axis, verify, convert_indices)\u001b[0m\n\u001b[1;32m    975\u001b[0m     indexer \u001b[39m=\u001b[39m maybe_convert_indices(indexer, n, verify\u001b[39m=\u001b[39mverify)\n\u001b[1;32m    977\u001b[0m new_labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes[axis]\u001b[39m.\u001b[39mtake(indexer)\n\u001b[0;32m--> 978\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreindex_indexer(\n\u001b[1;32m    979\u001b[0m     new_axis\u001b[39m=\u001b[39;49mnew_labels,\n\u001b[1;32m    980\u001b[0m     indexer\u001b[39m=\u001b[39;49mindexer,\n\u001b[1;32m    981\u001b[0m     axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m    982\u001b[0m     allow_dups\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    983\u001b[0m     copy\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    984\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/leafcutter-sc/lib/python3.9/site-packages/pandas/core/internals/managers.py:751\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[1;32m    749\u001b[0m     parent \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m com\u001b[39m.\u001b[39mall_none(\u001b[39m*\u001b[39mnew_refs) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[1;32m    750\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 751\u001b[0m     new_blocks \u001b[39m=\u001b[39m [\n\u001b[1;32m    752\u001b[0m         blk\u001b[39m.\u001b[39mtake_nd(\n\u001b[1;32m    753\u001b[0m             indexer,\n\u001b[1;32m    754\u001b[0m             axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m    755\u001b[0m             fill_value\u001b[39m=\u001b[39m(\n\u001b[1;32m    756\u001b[0m                 fill_value \u001b[39mif\u001b[39;00m fill_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m blk\u001b[39m.\u001b[39mfill_value\n\u001b[1;32m    757\u001b[0m             ),\n\u001b[1;32m    758\u001b[0m         )\n\u001b[1;32m    759\u001b[0m         \u001b[39mfor\u001b[39;00m blk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks\n\u001b[1;32m    760\u001b[0m     ]\n\u001b[1;32m    761\u001b[0m     new_refs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    762\u001b[0m     parent \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/leafcutter-sc/lib/python3.9/site-packages/pandas/core/internals/managers.py:752\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    749\u001b[0m     parent \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m com\u001b[39m.\u001b[39mall_none(\u001b[39m*\u001b[39mnew_refs) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[1;32m    750\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    751\u001b[0m     new_blocks \u001b[39m=\u001b[39m [\n\u001b[0;32m--> 752\u001b[0m         blk\u001b[39m.\u001b[39;49mtake_nd(\n\u001b[1;32m    753\u001b[0m             indexer,\n\u001b[1;32m    754\u001b[0m             axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m    755\u001b[0m             fill_value\u001b[39m=\u001b[39;49m(\n\u001b[1;32m    756\u001b[0m                 fill_value \u001b[39mif\u001b[39;49;00m fill_value \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m blk\u001b[39m.\u001b[39;49mfill_value\n\u001b[1;32m    757\u001b[0m             ),\n\u001b[1;32m    758\u001b[0m         )\n\u001b[1;32m    759\u001b[0m         \u001b[39mfor\u001b[39;00m blk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks\n\u001b[1;32m    760\u001b[0m     ]\n\u001b[1;32m    761\u001b[0m     new_refs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    762\u001b[0m     parent \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/leafcutter-sc/lib/python3.9/site-packages/pandas/core/internals/blocks.py:880\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[1;32m    877\u001b[0m     allow_fill \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    879\u001b[0m \u001b[39m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[0;32m--> 880\u001b[0m new_values \u001b[39m=\u001b[39m algos\u001b[39m.\u001b[39;49mtake_nd(\n\u001b[1;32m    881\u001b[0m     values, indexer, axis\u001b[39m=\u001b[39;49maxis, allow_fill\u001b[39m=\u001b[39;49mallow_fill, fill_value\u001b[39m=\u001b[39;49mfill_value\n\u001b[1;32m    882\u001b[0m )\n\u001b[1;32m    884\u001b[0m \u001b[39m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[1;32m    885\u001b[0m \u001b[39m#  this assertion\u001b[39;00m\n\u001b[1;32m    886\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m (axis \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m new_mgr_locs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/leafcutter-sc/lib/python3.9/site-packages/pandas/core/array_algos/take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39mtake(indexer, fill_value\u001b[39m=\u001b[39mfill_value, allow_fill\u001b[39m=\u001b[39mallow_fill)\n\u001b[1;32m    116\u001b[0m arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(arr)\n\u001b[0;32m--> 117\u001b[0m \u001b[39mreturn\u001b[39;00m _take_nd_ndarray(arr, indexer, axis, fill_value, allow_fill)\n",
      "File \u001b[0;32m~/miniconda3/envs/leafcutter-sc/lib/python3.9/site-packages/pandas/core/array_algos/take.py:134\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m     indexer \u001b[39m=\u001b[39m ensure_platform_int(indexer)\n\u001b[0;32m--> 134\u001b[0m dtype, fill_value, mask_info \u001b[39m=\u001b[39m _take_preprocess_indexer_and_fill_value(\n\u001b[1;32m    135\u001b[0m     arr, indexer, fill_value, allow_fill\n\u001b[1;32m    136\u001b[0m )\n\u001b[1;32m    138\u001b[0m flip_order \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[39mif\u001b[39;00m arr\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m arr\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mf_contiguous:\n",
      "File \u001b[0;32m~/miniconda3/envs/leafcutter-sc/lib/python3.9/site-packages/pandas/core/array_algos/take.py:576\u001b[0m, in \u001b[0;36m_take_preprocess_indexer_and_fill_value\u001b[0;34m(arr, indexer, fill_value, allow_fill, mask)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    575\u001b[0m     mask \u001b[39m=\u001b[39m indexer \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m--> 576\u001b[0m     needs_masking \u001b[39m=\u001b[39m \u001b[39mbool\u001b[39m(mask\u001b[39m.\u001b[39;49many())\n\u001b[1;32m    577\u001b[0m mask_info \u001b[39m=\u001b[39m mask, needs_masking\n\u001b[1;32m    578\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m needs_masking:\n\u001b[1;32m    579\u001b[0m     \u001b[39m# if not, then depromote, set fill_value to dummy\u001b[39;00m\n\u001b[1;32m    580\u001b[0m     \u001b[39m# (it won't be used but we don't want the cython code\u001b[39;00m\n\u001b[1;32m    581\u001b[0m     \u001b[39m# to crash when trying to cast it to dtype)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/leafcutter-sc/lib/python3.9/site-packages/numpy/core/_methods.py:57\u001b[0m, in \u001b[0;36m_any\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_any\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m, where\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m     55\u001b[0m     \u001b[39m# Parsing keyword arguments is currently fairly slow, so avoid it for now\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[39mif\u001b[39;00m where \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m         \u001b[39mreturn\u001b[39;00m umr_any(a, axis, dtype, out, keepdims)\n\u001b[1;32m     58\u001b[0m     \u001b[39mreturn\u001b[39;00m umr_any(a, axis, dtype, out, keepdims, where\u001b[39m=\u001b[39mwhere)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 3462 unique cells in the concatenated_df\n",
    "results = list()\n",
    "\n",
    "for cell in tqdm(concatenated_df.cell_id.unique()):\n",
    "    # get all pairs from all_cell_pairs that begin with cell\n",
    "    cell_pairs = [pair for pair in all_cell_pairs if pair[0] == cell]\n",
    "    # for each subpair in cell_pairs, check if they get assigned together across iterations in concatenated_df\n",
    "    for subpair in cell_pairs:\n",
    "        # get data \n",
    "        test=concatenated_df[concatenated_df[\"cell_id\"].isin([subpair[0], subpair[1]])].groupby([\"iteration\"]).cluster_id.unique()\n",
    "        # check if in both iterations, the length is 1 (that means they are in the same cluster)\n",
    "        res = test.apply(lambda x: len(x) == 1).unique()\n",
    "        # append subpair and res to a list\n",
    "        results.append((subpair, res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = np.argmax([ g[-1][-1] for g in results ]) # final ELBO\n",
    "ALPHA_f, PI_f, GAMMA_f, PHI_f, elbos_all = results[best]\n",
    "elbos_all = np.array(elbos_all)\n",
    "plt.plot(elbos_all[1:]); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juncs_probs = ALPHA_f / (ALPHA_f+PI_f)    \n",
    "plt.hist(juncs_probs.cpu().numpy().flatten(), 20); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHI_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_f_plot = pd.DataFrame(PHI_f.cpu().numpy())\n",
    "theta_f_plot['cell_id'] = cell_ids_conversion[\"cell_type\"].to_numpy()\n",
    "theta_f_plot_summ = theta_f_plot.groupby('cell_id').mean()\n",
    "print(theta_f_plot_summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = GAMMA_f / GAMMA_f.sum()\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much each cluster is used in each \"cell state\"\n",
    "#latent proportions describe the general prevalence of each cluster in the datasetN\n",
    "\n",
    "theta = GAMMA_f / GAMMA_f.sum()\n",
    "theta = theta.cpu().numpy()\n",
    "theta_sorted = np.sort(theta)\n",
    "plt.bar(np.arange(K)+1,theta_sorted[::-1]); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_keep = theta > 0.01\n",
    "\n",
    "x = PHI_f.cpu().numpy()\n",
    "x = x[:,to_keep]\n",
    "#x -= x.mean(1,keepdims=True)\n",
    "#x /= x.std(1,keepdims=True)\n",
    "_ = plt.hist(x.flatten(),100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHI_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = pd.crosstab( cell_ids_conversion[\"cell_type\"], x.argmax(axis=1) )\n",
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = pd.crosstab( cell_ids_conversion[\"cell_type\"], x.argmax(axis=1) )\n",
    "ct_np = ct.to_numpy()\n",
    "ct_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_np = ct_np / ct_np.sum(1, keepdims=True) # normalize cell-type counts\n",
    "ct_np = ct_np / ct_np.sum(0, keepdims=True)\n",
    "\n",
    "ct.iloc[:,:] = ct_np\n",
    "\n",
    "ax = plt.figure(figsize=[10,8])\n",
    "sns.clustermap(ct, dendrogram_ratio=0.15, vmin = None, figsize=(12,6), annot = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juncs_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juncs_probs_df = pd.DataFrame(juncs_probs, columns = range(K))\n",
    "# add \"cell_state\" to each column name \n",
    "juncs_probs_df.columns = [\"cell_state_\" + str(col) for col in juncs_probs_df.columns]\n",
    "juncs_probs_df[\"junction_id_index\"] = junction_ids_conversion.junction_id_index.values\n",
    "# convert to juncs_probs to pandas dataframe and calculate mean and std across cell states/topics\n",
    "juncs_probs_df[\"junction_id\"] = junction_ids_conversion.junction_id.values\n",
    "\n",
    "def plot_juncObsUsage(junc_index):\n",
    "\n",
    "    # print junction ID using junction_ids_conversion\n",
    "    print(junction_ids_conversion[junction_ids_conversion[\"junction_id_index\"] == junc_index])\n",
    "    junc_id = junction_ids_conversion[junction_ids_conversion[\"junction_id_index\"] == junc_index].junction_id.values[0]\n",
    "\n",
    "    # get data for just junc_index \n",
    "    junc_dat=final_data[final_data.junction_id_index==junc_index]\n",
    "    print(junc_dat.cell_type.value_counts())\n",
    "\n",
    "    # make violin plot for junc_dat junction usage ratio coloured by cell_type and rotate plot 90 degrees\n",
    "    plot = ggplot(junc_dat, aes(x='cell_type', y='juncratio', fill=\"cell_type\")) + geom_violin() + geom_point() + plotnine.labels.ggtitle(junc_id) + plotnine.coords.coord_flip() \n",
    "\n",
    "    # add number of cells in each cell_type to plot \n",
    "    print(plot)\n",
    "\n",
    "def plot_juncProbs(junc_index):\n",
    "    \n",
    "    # print junction ID using junction_ids_conversion\n",
    "    print(junction_ids_conversion[junction_ids_conversion[\"junction_id_index\"] == junc_index])\n",
    "    junc_id = junction_ids_conversion[junction_ids_conversion[\"junction_id_index\"] == junc_index].junction_id.values[0]\n",
    "    \n",
    "    # get data for just junc_index \n",
    "    junc_dat=juncs_probs_df[juncs_probs_df.junction_id_index==junc_index]\n",
    "    junc_dat = junc_dat.melt().iloc[0:K]\n",
    "    junc_dat.value = junc_dat.value.astype(float)\n",
    "    # make violin plot for junc_dat junction usage ratio coloured by cell_type\n",
    "    # don't print x-axis tick labels \n",
    "    plot = ggplot(junc_dat, aes(x='variable', y='value')) + geom_point() + theme(axis_text_x=element_blank())\n",
    "    print(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate sd deviation for each junction for cell states 0 to 19 \n",
    "juncs_probs_df[\"sd\"] = juncs_probs_df.iloc[:,0:K].std(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write function that takes in cell state and returns top 10 junctions with the highest difference with all other K-1 cell states\n",
    "def top10_juncs(cell_state):\n",
    "    # for each junction get the difference between cell_state and all other cell states not including cell_state\n",
    "    # return top 10 junctions with highest difference\n",
    "    no_ref=juncs_probs_df[juncs_probs_df.columns[~juncs_probs_df.columns.isin([cell_state, \"junction_id_index\", \"junction_id\", \"sd\"])]]\n",
    "    juncs_probs_df[\"diff\"] = juncs_probs_df[cell_state] - no_ref.mean(axis=1)\n",
    "    top10 = juncs_probs_df.sort_values(by=\"diff\", ascending=False).head(10)\n",
    "    return(top10.junction_id_index.values)\n",
    "\n",
    "    # think of actually using the distributions... use the full beta distribution via KL divergence... (pairwise)\n",
    "    # are the distributions across cell states for junctions more different than if they were coming from the same cell state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_beta(a, b):\n",
    "    return torch.lgamma(a) + torch.lgamma(b) - torch.lgamma(a + b)\n",
    "\n",
    "def score(a, b):\n",
    "    return log_beta(a,b).sum() - log_beta(a.sum(), b.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juncs_probs[0, [0,9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get likelihood ratio/bayes factor score for ALL junctions \n",
    "# let's compare just state 1 and 2 \n",
    "\n",
    "scores_all_juncs = []\n",
    "for junc_index in range(juncs_probs.shape[0]):\n",
    "    a = ALPHA_f[junc_index, [0,9]]\n",
    "    b = PI_f[junc_index, [0,9]]\n",
    "    scores_all_juncs.append(score(a, b).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn scores_all_juncs into dataframe and add junction_id_index as a column\n",
    "scores_all_juncs_df = pd.DataFrame(scores_all_juncs, columns = [\"score\"])\n",
    "scores_all_juncs_df[\"junction_id_index\"] = junction_ids_conversion.junction_id_index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_all_juncs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_all_juncs_df.sort_values(by=\"score\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juncs_test=scores_all_juncs_df.sort_values(by=\"score\", ascending=False).head(10).junction_id_index.values\n",
    "juncs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make histogram of scores\n",
    "scores_all_juncs_df.hist(column=\"score\", bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top10juncs_state1 = top10_juncs(\"cell_state_9\")\n",
    "#top10juncs_state1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each junction in top10juncs_state1, run plot_juncObsUsage and plot_juncProbs\n",
    "for junc in juncs_test:\n",
    "    plot_juncObsUsage(junc)\n",
    "    plot_juncProbs(junc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leafcutter-sc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
