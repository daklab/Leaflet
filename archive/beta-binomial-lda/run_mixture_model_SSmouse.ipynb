{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from load_cluster_data import load_cluster_data\n",
    "from pca_kmeans_init import pca_kmeans_init\n",
    "from betabinomo_mix_singlecells import *\n",
    "import betabinomo_mix_singlecells\n",
    "reload(betabinomo_mix_singlecells)\n",
    "import torch\n",
    "import sklearn.manifold \n",
    "import plotnine as p9\n",
    "import time\n",
    "# indicate plot should be small 4 by 4\n",
    "import plotnine as p9\n",
    "from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap, geom_violin, theme, element_blank, geom_text\n",
    "import plotnine\n",
    "from tqdm import tqdm\n",
    "plotnine.options.figure_size = (4, 4)\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings and Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '/gpfs/commons/scratch/kisaev/ss_tabulamuris_test/Leaflet/clustered_junctions_noanno.txt_anno_free_50_500000_10_5_0.1_single_cell.h5'\n",
    "\n",
    "# this folder contains input data for each tissue cell type sample\n",
    "input_files_folder = '/gpfs/commons/scratch/kisaev/ss_tabulamuris_test/Leaflet/Marrow/'\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "MAKE_PCA_TSNE = True\n",
    "\n",
    "float_type = { \n",
    "    \"device\" : device, \n",
    "    \"dtype\" : torch.float, # save memory\n",
    "}\n",
    "\n",
    "hypers = {\n",
    "    \"eta\" : 1., \n",
    "    \"alpha_prior\" : 1., # karin had 0.65 \n",
    "    \"pi_prior\" : 1.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data, coo_counts_sparse, coo_cluster_sparse, cell_ids_conversion, junction_ids_conversion = load_cluster_data(\n",
    "    input_folder = input_files_folder) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = coo_cluster_sparse.shape[0]\n",
    "J = coo_cluster_sparse.shape[1]\n",
    "\n",
    "cell_index_tensor, junc_index_tensor, my_data = betabinomo_mix_singlecells.make_torch_data(final_data, **float_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 5 # should also be an argument that gets fed in\n",
    "num_iters = 35 # should also be an argument that gets fed in\n",
    "K = 22\n",
    "\n",
    "# loop over the number of trials (for now just testing using one trial but in general need to evaluate how performance is affected by number of trials)\n",
    "reload(betabinomo_mix_singlecells)\n",
    "\n",
    "start_time = time.time()\n",
    "results = [ betabinomo_mix_singlecells.calculate_CAVI(K, my_data, float_type, hypers, init_labels = None, num_iterations = num_iters) \n",
    "           for t in range(num_trials) ]\n",
    "\n",
    "# write the above line use fstring\n",
    "print(f\"This took {time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### -> why some iterations converge after 20 iterations and some go on for 100 iterations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check how consistently cells get co-assigned together across iterations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract PHI_f from every trial in num_trials\n",
    "all_iters_PHI_f = [ result[3] for result in results ]\n",
    "i = 0\n",
    "\n",
    "# Create an empty list to store DataFrames from each iteration\n",
    "dfs_list = []\n",
    "\n",
    "for PHI_var in all_iters_PHI_f:\n",
    "\n",
    "    probability_tensor = PHI_var\n",
    "\n",
    "    # Create an array with cell IDs (e.g., cell_0, cell_1, ..., cell_(N-1))\n",
    "    cell_ids = np.arange(probability_tensor.shape[0])\n",
    "    cell_ids = [cell_id for cell_id in cell_ids]\n",
    "\n",
    "    # Get the cluster IDs for each cell based on the maximum probability\n",
    "    cluster_ids = np.argmax(probability_tensor, axis=1)\n",
    "\n",
    "    # Create a DataFrame with the cell_id, cluster_id, and probability columns\n",
    "    df = pd.DataFrame({\"cell_id\": cell_ids, \"cluster_id\": cluster_ids})\n",
    "\n",
    "    # Add column with iteration number\n",
    "    df[\"iteration\"] = i\n",
    "    i += 1\n",
    "    # Append the DataFrame to the list\n",
    "    dfs_list.append(df)\n",
    "\n",
    "# Concatenate all the DataFrames into a single DataFrame\n",
    "concatenated_df = pd.concat(dfs_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate list to save results for each iteration\n",
    "\n",
    "all_iters_results = [None] * num_trials\n",
    "\n",
    "for trial in range(num_trials):\n",
    "    # Find the unique clusters for each cell_id\n",
    "    cell_by_cell_matrix = np.zeros((N, N))\n",
    "\n",
    "    clusters = concatenated_df.loc[concatenated_df[\"iteration\"] == trial, [\"cell_id\", \"cluster_id\"]]\n",
    "    unique_clusters = clusters.set_index('cell_id')['cluster_id'].to_dict()\n",
    "\n",
    "    # Fill the cell_by_cell_matrix using numpy indexing\n",
    "    for cell_id, cluster_id in unique_clusters.items():\n",
    "        cell_by_cell_matrix[cell_id, cell_id] = 1\n",
    "        same_cluster_cells = clusters[clusters[\"cluster_id\"] ==  cluster_id].cell_id.values\n",
    "        cell_by_cell_matrix[cell_id, same_cluster_cells] = 1\n",
    "\n",
    "    all_iters_results[trial] = cell_by_cell_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all pairs of num_trials \n",
    "import itertools\n",
    "all_pairs = list(itertools.combinations(range(num_trials), 2))\n",
    "# Create an empty list to store DataFrames from each iteration\n",
    "dfs_list = []\n",
    "\n",
    "for pair in tqdm(all_pairs):\n",
    "    ## assess similarity between iterations\n",
    "    x = (all_iters_results[pair[0]] - all_iters_results[pair[1]])\n",
    "    unique, counts = np.unique(x, return_counts=True)\n",
    "    # turn unique, counts into dataframe \n",
    "    df = pd.DataFrame({'unique': unique, 'counts': counts})\n",
    "    # get percentage for counts \n",
    "    df['percentage'] = df['counts']/df['counts'].sum()\n",
    "    df[\"pair\"] = str(pair)\n",
    "    dfs_list.append(df)\n",
    "\n",
    "# Concatenate all the DataFrames into a single DataFrame\n",
    "concatenated_iters_comp = pd.concat(dfs_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_iters_comp = concatenated_iters_comp[[\"unique\", \"pair\", \"percentage\"]]\n",
    "\n",
    "# turn into wide format for plotting heatmap\n",
    "concatenated_iters_comp_wide = concatenated_iters_comp.pivot(index='pair', columns='unique', values='percentage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_iters_comp.sort_values(by=['percentage'], inplace=True, ascending=False)\n",
    "\n",
    "print(f\"The minimum percentage of matching cell pairs across all trials is {concatenated_iters_comp[concatenated_iters_comp['unique'] == 0]['percentage'].min().round(2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenated_iters_comp\n",
    "\n",
    "# use seaborn to plot the results, x-axis can be the pair and y-axis can be the value in the unique column \n",
    "g = sns.clustermap(concatenated_iters_comp_wide, cmap=\"crest\", linewidth=.1, figsize=(4, 6), yticklabels=1)\n",
    "g.ax_col_dendrogram.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the learned posterions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = np.argmax([ g[-1][-1] for g in results ]) # final ELBO\n",
    "print(f\"The trial with the highest ELBO was {best}\")\n",
    "\n",
    "# print rows that contains best in the pair and only where unique == 0\n",
    "best_res_comp = (concatenated_iters_comp[concatenated_iters_comp.pair.str.contains(str(best))])\n",
    "print(best_res_comp[best_res_comp['unique'] == 0])\n",
    "\n",
    "print(f\"The pair with the highest percentage overlap is {concatenated_iters_comp.iloc[0].pair}\")\n",
    "print(f\"The highest percentage overlap across pairs was {concatenated_iters_comp.iloc[0].percentage.round(3)}\")\n",
    "\n",
    "ALPHA_f, PI_f, GAMMA_f, PHI_f, elbos_all = results[best]\n",
    "elbos_all = np.array(elbos_all)\n",
    "plt.plot(elbos_all[1:]); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juncs_probs = ALPHA_f / (ALPHA_f+PI_f)    \n",
    "plt.hist(juncs_probs.cpu().numpy().flatten(), 20); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_f_plot = pd.DataFrame(PHI_f.cpu().numpy())\n",
    "theta_f_plot['cell_id'] = cell_ids_conversion[\"cell_type\"].to_numpy()\n",
    "theta_f_plot_summ = theta_f_plot.groupby('cell_id').mean()\n",
    "print(theta_f_plot_summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.dirichlet(GAMMA_f, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA_f / GAMMA_f.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much each cell state is used \n",
    "#latent proportions describe the general prevalence of each cluster in the datasetN\n",
    "# cluster proportions via theta ~ dirichlet(GAMMA_f)\n",
    "# each cell gets an assignment to a cluster via z_c | theta ~ categorical(theta)\n",
    "\n",
    "theta = GAMMA_f / GAMMA_f.sum()\n",
    "theta = theta.cpu().numpy()\n",
    "theta_sorted = np.sort(theta)\n",
    "theta_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHI_f #<- this is the matrix of probabilities of each cell belonging to each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(np.arange(K)+1,theta_sorted[::-1]); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_keep = theta > 0.01\n",
    "\n",
    "x = PHI_f.cpu().numpy()\n",
    "x = x[:,to_keep]\n",
    "#x -= x.mean(1,keepdims=True)\n",
    "#x /= x.std(1,keepdims=True)\n",
    "_ = plt.hist(x.flatten(),100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = pd.crosstab(cell_ids_conversion[\"cell_type\"], x.argmax(axis=1) )\n",
    "print(ct)\n",
    "\n",
    "ct_np = ct.to_numpy()\n",
    "print(ct_np)\n",
    "\n",
    "ct_np = ct_np / ct_np.sum(1, keepdims=True) # normalize cell-type counts\n",
    "print(ct_np)\n",
    "\n",
    "ct_np = ct_np / ct_np.sum(0, keepdims=True)\n",
    "print(ct_np)\n",
    "\n",
    "ct.iloc[:,:] = ct_np\n",
    "\n",
    "ax = plt.figure(figsize=[10,8])\n",
    "sns.clustermap(ct, dendrogram_ratio=0.15, vmin = None, figsize=(12,6), annot = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The dataset used to generate the heatmap above likely has a bug**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juncs_probs_df = pd.DataFrame(juncs_probs, columns = range(K))\n",
    "# add \"cell_state\" to each column name \n",
    "juncs_probs_df.columns = [\"cell_state_\" + str(col) for col in juncs_probs_df.columns]\n",
    "juncs_probs_df[\"junction_id_index\"] = junction_ids_conversion.junction_id_index.values\n",
    "# convert to juncs_probs to pandas dataframe and calculate mean and std across cell states/topics\n",
    "juncs_probs_df[\"junction_id\"] = junction_ids_conversion.junction_id.values\n",
    "\n",
    "def plot_juncObsUsage(junc_index):\n",
    "\n",
    "    # print junction ID using junction_ids_conversion\n",
    "    print(junction_ids_conversion[junction_ids_conversion[\"junction_id_index\"] == junc_index])\n",
    "    junc_id = junction_ids_conversion[junction_ids_conversion[\"junction_id_index\"] == junc_index].junction_id.values[0]\n",
    "\n",
    "    # get data for just junc_index \n",
    "    junc_dat=final_data[final_data.junction_id_index==junc_index]\n",
    "    print(junc_dat.cell_type.value_counts())\n",
    "\n",
    "    # make violin plot for junc_dat junction usage ratio coloured by cell_type and rotate plot 90 degrees\n",
    "    plot = ggplot(junc_dat, aes(x='cell_type', y='juncratio', fill=\"cell_type\")) + geom_violin() + geom_point() + plotnine.labels.ggtitle(junc_id) + plotnine.coords.coord_flip() \n",
    "\n",
    "    # add number of cells in each cell_type to plot \n",
    "    print(plot)\n",
    "\n",
    "def plot_juncProbs(junc_index):\n",
    "    \n",
    "    # print junction ID using junction_ids_conversion\n",
    "    print(junction_ids_conversion[junction_ids_conversion[\"junction_id_index\"] == junc_index])\n",
    "    junc_id = junction_ids_conversion[junction_ids_conversion[\"junction_id_index\"] == junc_index].junction_id.values[0]\n",
    "    \n",
    "    # get data for just junc_index \n",
    "    junc_dat=juncs_probs_df[juncs_probs_df.junction_id_index==junc_index]\n",
    "    junc_dat = junc_dat.melt().iloc[0:K]\n",
    "    junc_dat.value = junc_dat.value.astype(float)\n",
    "    # make violin plot for junc_dat junction usage ratio coloured by cell_type\n",
    "    # don't print x-axis tick labels \n",
    "    plot = ggplot(junc_dat, aes(x='variable', y='value')) + geom_point() + theme(axis_text_x=element_blank())\n",
    "    print(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate sd deviation for each junction for cell states 0 to 19 \n",
    "juncs_probs_df[\"sd\"] = juncs_probs_df.iloc[:,0:K].std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write function that takes in cell state and returns top 10 junctions with the highest difference with all other K-1 cell states\n",
    "def top10_juncs(cell_state):\n",
    "    # for each junction get the difference between cell_state and all other cell states not including cell_state\n",
    "    # return top 10 junctions with highest difference\n",
    "    no_ref=juncs_probs_df[juncs_probs_df.columns[~juncs_probs_df.columns.isin([cell_state, \"junction_id_index\", \"junction_id\", \"sd\"])]]\n",
    "    juncs_probs_df[\"diff\"] = juncs_probs_df[cell_state] - no_ref.mean(axis=1)\n",
    "    top10 = juncs_probs_df.sort_values(by=\"diff\", ascending=False).head(10)\n",
    "    return(top10.junction_id_index.values)\n",
    "\n",
    "    # think of actually using the distributions... use the full beta distribution via KL divergence... (pairwise)\n",
    "    # are the distributions across cell states for junctions more different than if they were coming from the same cell state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_beta(a, b):\n",
    "    return torch.lgamma(a) + torch.lgamma(b) - torch.lgamma(a + b)\n",
    "\n",
    "def score(a, b):\n",
    "    return log_beta(a,b).sum() - log_beta(a.sum(), b.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get likelihood ratio/bayes factor score for ALL junctions \n",
    "# let's compare just state X and Y\n",
    "\n",
    "scores_all_juncs = []\n",
    "for junc_index in range(juncs_probs.shape[0]):\n",
    "    a = ALPHA_f[junc_index, [3,7]]\n",
    "    b = PI_f[junc_index, [3,7]]\n",
    "    scores_all_juncs.append(score(a, b).item())\n",
    "\n",
    "# turn scores_all_juncs into dataframe and add junction_id_index as a column\n",
    "scores_all_juncs_df = pd.DataFrame(scores_all_juncs, columns = [\"score\"])\n",
    "scores_all_juncs_df[\"junction_id_index\"] = junction_ids_conversion.junction_id_index.values\n",
    "scores_all_juncs_df.sort_values(by=\"score\", ascending=False).head(10)\n",
    "juncs_test=scores_all_juncs_df.sort_values(by=\"score\", ascending=False).head(5).junction_id_index.values\n",
    "scores_all_juncs_df.hist(column=\"score\", bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each junction in top10juncs_state1, run plot_juncObsUsage and plot_juncProbs\n",
    "for junc in juncs_test:\n",
    "    plot_juncObsUsage(junc)\n",
    "    plot_juncProbs(junc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_ids_conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert PHI_f to a dataframe and add a column with cell ID and cell type \n",
    "PHI_f = pd.DataFrame(PHI_f)\n",
    "\n",
    "# Add \"CellState\" to each column \n",
    "PHI_f.columns = [\"CellState_\" + str(i) for i in range(PHI_f.shape[1])]\n",
    "PHI_f['cell_id'] = cell_ids_conversion.cell_id.values\n",
    "PHI_f['cell_type'] = cell_ids_conversion.cell_type.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHI_f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cell assignments to file for downstream analysis\n",
    "\n",
    "output_dir = '/gpfs/commons/groups/knowles_lab/Karin/Leaflet-analysis-WD/TabulaMurisMarrow'\n",
    "\n",
    "# BBmixture model cell assignments\n",
    "output_file = os.path.join(output_dir, 'Leaflet_BBmixture.csv')\n",
    "PHI_f.to_csv(output_file, index=True, header=True)\n",
    "print('Saved Leaflet latent cell states to {}'.format(output_file))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leafcutter-sc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
